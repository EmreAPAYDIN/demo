{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensor_and_Gradients.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNKtDQic4BnKjOXBzBlO1SB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmreAPAYDIN/phyton/blob/master/Tensor_and_Gradients.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG29rGO_UDCu"
      },
      "source": [
        "# 1.10.1 Tensor and Gradients\n",
        "\n",
        "The tensor is the most fundamental computation unit in TensorFlow and it is used  to represent outputs of an operation. A tensor can be created by operations such  as **tf.constant**, **tf.matmul**, etc. Tensor does not store the values of the  operationâ€™s outputs but provides access to the computation of those values in a  TensorFlow session. In TensorFlow 2.0, there is no need to run a session manually,  as in eager execution, graphs and sessions are designed to stay in the backend. For  examples, in the matrix multiplication as shown below, matrices can be created by  **tf.constant** and the multiplication is computed by **tf.matmul** whose output  is another matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoErnoKxURKW"
      },
      "source": [
        "# Matrix multiplication in TensorFlow by Tensor\n",
        "import tensorflow as tf\n",
        "a = tf.constant([[1, 2], [1, 2]])\n",
        "# tf.Tensor(\n",
        "# [[1 2]\n",
        "# [1 2]], shape=(2, 2), dtype=int32)\n",
        "b = tf.constant([[1], [2]])\n",
        "# tf.Tensor(\n",
        "# [[1]\n",
        "# [2]], shape=(2, 1), dtype=int32)\n",
        "c = tf.matmul(a, b)\n",
        "# tf.Tensor(\n",
        "# [[5]\n",
        "# [5]], shape=(2, 1), dtype=int32)\n",
        "print(c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hYq3ESvI_Pm"
      },
      "source": [
        "In the forward propagation of deep neural networks, the tensors are automatically connected by each other as a graph. Based on the graph and the automaticdifferentiation technique provided by TensorFlow, gradients can be computed in  the back-propagation. TensorFlow 2.0 provides tf.GradientTape to compute  gradients of recorded operations with respect to its input variables. For example,  the code below shows an example of computing gradients in back-propagation.  The forward propagation and the computation of loss are within the scope of  tf.GradientTape, while the back-propagation and the update of weights  are outside the scope. The tf.GradientTape records all operations that are  executed within the scope onto a tape. Then the gradients associated with each  recorded operation and its input variables are computed by reverse-mode automatic  differentiation. Once the function tape.gradient() is called, the resources  held by tf.GradientTape are released."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5t6Xes9EMvpb"
      },
      "source": [
        "!pip install tensorlayer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wn07EW6XLIdK"
      },
      "source": [
        "#Gradients computation in TensorFlow and TensorLayer.\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorlayer as tl\n",
        "def train(model, dataset, optimizer):\n",
        "# given a model which is an instance of Model by TensorLayer\n",
        "# traverse the dataset where x is input and y is target output\n",
        "  for x, y in dataset:\n",
        "    # create the scope of gradient tape\n",
        "    with tf.GradientTape() as tape:\n",
        "      prediction = model(x) # forward propagation\n",
        "      loss = loss_fn(prediction, y) # loss function\n",
        "    # back-propagation and computing gradients, then the resources held by the GradientTape are released\n",
        "    gradients = tape.gradient(loss, model.trainable_weights)\n",
        "    # apply the gradients to weights and update the weights by the optimizer\n",
        "    optimizer.apply_gradients(zip(gradients,model.trainable_weights))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rNorxjrPcCO"
      },
      "source": [
        "# 1.10.2 Define a Model\n",
        "\n",
        "In TensorLayer 2.0, Model is an entity that consists of multiple Layers and defines  the propagation between the Layers. TensorLayer 2.0 provides two sets of APIs  to define a model. Static model APIs allow users to build a model fluently and  dynamic model APIs provide more flexibility in the forward propagation. A static  model requires users to manually construct a graph and compile it. Once the model  is compiled, the forward propagation cannot be changed. Unlike the static model, the  dynamic model can be executed eagerly like Python normally does and the forward  propagation is mutable.\n",
        "\n",
        "In the implementation of models, as shown in the examples below, the difference between a static model and a dynamic model can be summarized in two aspects. First, when layers in a static model are declared, the connection between layers (i.e., the forward propagation) is defined explicitly at the same time. Based on the  connection, for each layer, TensorLayer can automatically infer the size of input  variables from previous layers and then construct weights. When the Model is  finally instanced, only inputs and outputs need to be specified and TensorLayer  automatically builds a graph based on the connection. However, when a dynamic  model is initialized, the forward propagation is still unknown until it is defined in the  function forward later. Thus, the size of input variables cannot be automatically  inferred and it has to been manually provided via the argument in_channels.\n",
        "\n",
        "Second, the forward propagation of a static model is fixed once the model is constructed, so it is easier to accelerate the computation of a static model. TensorFlow  2.0 provides a new feature called tf.function which can be used as a decorator  and accelerate the computation. Unlike the static model, the forward propagation  in a dynamic model can be more flexible. For example, the forward flow can be  controlled based on input values or arguments specified by users. Users are also  allowed to use or abandon any layer in the forward propagation of a dynamic model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVvVbaWhYsdG"
      },
      "source": [
        "!pip install --upgrade tensorlayer[all]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqLePMUmPcou"
      },
      "source": [
        "# An example of a static model: multilayer perceptron (MLP)\n",
        "import tensorflow as tf\n",
        "from tensorlayer.layers import Input, Dense\n",
        "from tensorlayer.models import Model\n",
        "# a multilayer perceptron (MLP) model with three dense layers\n",
        "def get_mlp_model(inputs_shape):\n",
        "  ni = Input(inputs_shape)\n",
        "  # since the connection between layers is explicitly defined\n",
        "  # in_channels of each layer is automatically inferred\n",
        "  nn = Dense(n_units=800, act=tf.nn.relu)(ni)\n",
        "  nn = Dense(n_units=800, act=tf.nn.relu)(nn)\n",
        "  nn = Dense(n_units=10, act=tf.nn.relu)(nn)\n",
        "  # automatic build a model based on the connection between layers\n",
        "  M = Model(inputs=ni,outputs=nn)\n",
        "  return M\n",
        "MLP = get_mlp_model([None, 784])\n",
        "# switch to evaluation mode\n",
        "MLP.eval()\n",
        "# ingest data into the model\n",
        "# the computation can be accelerated by using @tf.function in TensorFlow 2.0\n",
        "outputs = MLP(data)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}